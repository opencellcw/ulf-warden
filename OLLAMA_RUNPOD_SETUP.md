# üî• Ollama Uncensored no RunPod + Ulf no Render

## Arquitetura

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ    Slack    ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ Ulf (Render) ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ Ollama (RunPod GPU) ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                            ‚îÇ
                            ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂ Claude API (backup)
```

**Vantagens:**
- ‚úÖ Ollama roda em GPU potente (RunPod)
- ‚úÖ Ulf roda 24/7 barato (Render free tier)
- ‚úÖ Sem censura nos modelos locais
- ‚úÖ Fallback para Claude em tasks complexas

---

## üì¶ PARTE 1: Setup Ollama no RunPod

### 1.1 Criar Pod no RunPod

1. Acesse [runpod.io](https://runpod.io)
2. **Deploy** ‚Üí **GPU Pods**
3. Escolha template:
   - **PyTorch** ou **Ubuntu + CUDA**
   - GPU: **RTX 3090** (24GB VRAM) ou **A4000** (16GB VRAM)
   - Custo: ~$0.30-0.50/hora
4. Configurar:
   - Volume persistente: **20GB** (para modelo + cache)
   - Expose HTTP ports: `11434`
   - Container Disk: **50GB**

### 1.2 Instalar Ollama no RunPod

Conecte via SSH ou Web Terminal:

```bash
# 1. Instalar Ollama
curl -fsSL https://ollama.com/install.sh | sh

# 2. Iniciar Ollama (expor para rede externa)
OLLAMA_HOST=0.0.0.0:11434 ollama serve &

# 3. Baixar modelo uncensored
ollama pull wizard-vicuna-uncensored:7b

# Ou modelo maior (precisa mais VRAM):
# ollama pull wizard-vicuna-uncensored:13b

# 4. Testar
ollama run wizard-vicuna-uncensored:7b "Oi, voc√™ est√° funcionando?"
```

### 1.3 Manter Ollama Rodando (PM2)

```bash
# Instalar PM2
npm install -g pm2

# Criar script de inicializa√ß√£o
cat > ~/start-ollama.sh << 'EOF'
#!/bin/bash
export OLLAMA_HOST=0.0.0.0:11434
ollama serve
EOF

chmod +x ~/start-ollama.sh

# Rodar com PM2
pm2 start ~/start-ollama.sh --name ollama

# Auto-start no boot
pm2 startup
pm2 save

# Verificar status
pm2 status
pm2 logs ollama
```

### 1.4 Expor Ollama para Internet

**Op√ß√£o A: Usar RunPod Proxy (mais f√°cil)**

RunPod automaticamente exp√µe portas. Anote a URL:
```
https://<pod-id>-11434.proxy.runpod.net
```

**Op√ß√£o B: Cloudflare Tunnel (mais seguro)**

```bash
# Instalar cloudflared
wget https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64
chmod +x cloudflared-linux-amd64
sudo mv cloudflared-linux-amd64 /usr/local/bin/cloudflared

# Criar t√∫nel
cloudflared tunnel --url http://localhost:11434
# Anote a URL gerada (ex: https://random-name.trycloudflare.com)
```

---

## ü§ñ PARTE 2: Conectar Ulf (Render) ao Ollama (RunPod)

### 2.1 Configurar .env no Render

No dashboard do Render, adicione as vari√°veis:

```env
# Ollama remoto (RunPod)
OLLAMA_ENABLED=true
OLLAMA_BASE_URL=https://<pod-id>-11434.proxy.runpod.net
OLLAMA_MODEL=wizard-vicuna-uncensored:7b

# Ou se usar Cloudflare:
# OLLAMA_BASE_URL=https://random-name.trycloudflare.com

# Manter Claude como backup
ANTHROPIC_API_KEY=sk-ant-api03-xxx
```

### 2.2 Testar Conex√£o

Ap√≥s configurar, verifique os logs do Render:

```
[info]: [Router] Ollama availability checked {"available":true}
[info]: [Ollama] Connected to remote Ollama at https://...
```

### 2.3 Como Funciona

O router (`src/llm/router.ts`) decide automaticamente:

| Tipo de Task | LLM Usado |
|--------------|-----------|
| Chat simples, perguntas b√°sicas | **Ollama uncensored** üî• |
| C√≥digo, an√°lise, ferramentas | **Claude API** üß† |
| Ollama offline/erro | **Claude API** (fallback) |

---

## üß™ PARTE 3: Testar

### 3.1 No Slack

```
# Task simples (vai pro Ollama uncensored)
@Ulf me conta uma piada sem censura

# Task complexa (vai pro Claude)
@Ulf analisa esse c√≥digo e sugere melhorias: [c√≥digo]

# Com tool use (sempre Claude)
@Ulf gera uma imagem de uma espada viking
```

### 3.2 Verificar Logs

**No Render:**
```
[info]: [Router] Selected provider: ollama (task: simple_chat)
[info]: [Ollama] Generating response...
```

**No RunPod:**
```bash
pm2 logs ollama

# Deve mostrar requests chegando:
# POST /api/generate
```

---

## üí∞ Custos

| Servi√ßo | Custo |
|---------|-------|
| **Render** (Ulf bot) | Gr√°tis (free tier) |
| **RunPod** (Ollama GPU) | $0.30-0.50/hora (~$7-15/m√™s se 24/7) |
| **Claude API** | Pay-per-use (backup) |

**üí° Dica:** Configure auto-shutdown no RunPod quando n√£o estiver usando (hor√°rio noturno, finais de semana).

---

## üîß Troubleshooting

### Ollama n√£o conecta

```bash
# No RunPod, verificar se est√° rodando:
pm2 status

# Testar localmente no RunPod:
curl http://localhost:11434/api/tags

# Deve retornar lista de modelos
```

### Render n√£o alcan√ßa RunPod

```bash
# Verificar se porta est√° exposta:
# No dashboard RunPod: TCP Port Mappings ‚Üí 11434

# Testar URL p√∫blica:
curl https://<pod-id>-11434.proxy.runpod.net/api/tags
```

### Modelo muito lento

- Trocar para GPU mais potente (RTX 4090, A6000)
- Usar modelo menor: `wizard-vicuna-uncensored:7b` em vez de `:13b`
- Aumentar `num_gpu` no Ollama

### Out of Memory

```bash
# Verificar VRAM:
nvidia-smi

# Limpar cache:
ollama rm wizard-vicuna-uncensored:7b
ollama pull wizard-vicuna-uncensored:7b
```

---

## üöÄ Melhorias Futuras

### 1. Load Balancer (m√∫ltiplos RunPods)

Se tiver tr√°fego alto, rode m√∫ltiplas inst√¢ncias:

```env
OLLAMA_BASE_URL=https://runpod1.com,https://runpod2.com,https://runpod3.com
```

Adicionar round-robin em `src/llm/router.ts`.

### 2. Cache de Respostas

Adicionar Redis para cachear respostas comuns:

```typescript
// Evita chamar Ollama pra mesma pergunta
const cached = await redis.get(`ollama:${prompt_hash}`);
```

### 3. Monitoria

Adicionar health check:

```typescript
// src/llm/ollama.ts
async healthCheck(): Promise<boolean> {
  try {
    const response = await axios.get(`${this.baseUrl}/api/tags`);
    return response.status === 200;
  } catch {
    return false;
  }
}
```

---

## üìä Status Atual

Depois de configurado, verifique em `/health`:

```json
{
  "status": "healthy",
  "llm": {
    "ollama": {
      "available": true,
      "url": "https://xxx-11434.proxy.runpod.net",
      "model": "wizard-vicuna-uncensored:7b"
    },
    "claude": {
      "available": true,
      "fallback": true
    }
  }
}
```

---

## ‚úÖ Checklist de Setup

- [ ] RunPod Pod criado com GPU
- [ ] Ollama instalado no RunPod
- [ ] Modelo `wizard-vicuna-uncensored:7b` baixado
- [ ] Ollama exposto na porta 11434
- [ ] PM2 rodando Ollama (auto-restart)
- [ ] URL p√∫blica do RunPod anotada
- [ ] Vari√°veis de ambiente no Render configuradas
- [ ] Ulf reiniciado no Render
- [ ] Logs mostram `[Ollama] available: true`
- [ ] Teste no Slack funcionando

---

## üéØ Resultado Final

Agora voc√™ tem:
- ‚úÖ Ulf respondendo 24/7 (Render)
- ‚úÖ Ollama uncensored em GPU potente (RunPod)
- ‚úÖ Claude como backup para tasks complexas
- ‚úÖ Custo otimizado ($7-15/m√™s vs $100+ de API calls)
- ‚úÖ Sem censura, sem limites üî•

**Divirta-se com seu Ulf turbinado!** ‚öîÔ∏è
