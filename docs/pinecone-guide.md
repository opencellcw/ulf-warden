# ğŸ§  Pinecone Integration - Complete Guide

**Setup time:** 1-2 horas  
**ROI:** $2,400/year (infinite context + semantic search)  
**Free tier:** 1 index, 100k vectors, unlimited queries

---

## ğŸ¯ What is Pinecone?

Pinecone is a **vector database** that enables:
- ğŸ§  **Infinite memory** for bots (no context limits)
- ğŸ” **Semantic search** (find similar conversations)
- ğŸ’¾ **Long-term memory** (persists across restarts)
- âš¡ **Fast retrieval** (<100ms queries)

**Use cases:**
- Remember user preferences
- Find relevant past conversations
- RAG (Retrieval Augmented Generation)
- Smart context injection

---

## ğŸš€ Quick Setup (30 minutes)

### Step 1: Create Pinecone Account

1. **Sign up:** https://www.pinecone.io
2. **Create API key:**
   - Dashboard â†’ API Keys
   - Click "Create API Key"
   - Copy key

### Step 2: Configure OpenCell

Add to `.env`:
```bash
# Pinecone
PINECONE_ENABLED=true
PINECONE_API_KEY=xxx  # Your API key
PINECONE_INDEX=opencell-memory

# OpenAI (for embeddings)
OPENAI_API_KEY=xxx  # Required for vector generation
```

### Step 3: Create Index

```bash
# Run setup script
npx tsx scripts/setup-pinecone.ts

# Output:
# ğŸš€ Setting up Pinecone...
# âœ… Configuration validated
# ğŸ“¦ Creating index: opencell-memory
# âœ… Index created
# ğŸ§ª Testing with sample data...
# âœ… All tests passed! âœ…
```

**What this does:**
- Creates index (dimension: 1536, metric: cosine)
- Tests connection
- Stores & retrieves sample vector
- Validates setup

---

## ğŸ“š Usage Examples

### Basic Memory Storage

```typescript
import { memory } from './src/memory/vector-memory';

// Store conversation
await memory.storeMessage(
  'my-bot',      // botId
  'user-123',    // userId
  'user',        // role
  'I love pizza' // content
);

// Store assistant response
await memory.storeMessage(
  'my-bot',
  'user-123',
  'assistant',
  'Great! I\'ll remember that.'
);
```

### Retrieve Context

```typescript
// Get context for new message
const context = await memory.getContext(
  'my-bot',
  'user-123',
  'What food do I like?', // query
  3,                      // recent messages
  5                       // relevant memories
);

console.log(context.summary);
// Output:
// Recent conversation:
// - user: I love pizza
// - assistant: Great! I'll remember that.
// 
// Relevant context:
// - User prefers Italian food (95% relevant)
// - User is vegetarian (82% relevant)
```

### Search Memories

```typescript
// Semantic search
const memories = await memory.search(
  'my-bot',
  'food preferences',
  'user-123',
  5 // limit
);

memories.forEach(mem => {
  console.log(`${(mem.score * 100).toFixed(0)}%: ${mem.content}`);
});
// Output:
// 98%: I love pizza
// 85%: I'm vegetarian
// 72%: I don't like spicy food
```

---

## ğŸ¤– Automatic Integration

**Memory is automatically used in agent!**

When you send a message, the agent:
1. Retrieves relevant context from Pinecone
2. Injects it into system prompt
3. Stores user message + response

**No code changes needed!** Just enable Pinecone and set `botName` in options:

```typescript
const response = await runAgent({
  userId: 'user-123',
  userMessage: 'What do I like?',
  history: [],
  botName: 'my-bot', // This enables memory!
});
// Agent automatically remembers user preferences! âœ…
```

---

## ğŸ” How It Works

### 1. Embedding Generation

```typescript
import { embeddings } from './src/vector/embeddings';

// Convert text to vector (1536 dimensions)
const vector = await embeddings.embed('I love pizza');
// [0.1, -0.5, 0.3, ..., 0.2] (1536 numbers)
```

**Cost:** $0.00002 per 1k tokens (~750 words)  
**Speed:** ~100ms per embedding  
**Cache:** 24 hours (via Redis)

### 2. Vector Storage

```typescript
import { pinecone } from './src/vector/pinecone';

// Store in namespace (bot-specific)
await pinecone.storeMemory(
  'my-bot',
  'user-123',
  'I love pizza',
  vector,
  { category: 'food' }
);
```

### 3. Semantic Search

```typescript
// Find similar memories
const queryVector = await embeddings.embed('What food?');
const results = await pinecone.searchMemories(
  'my-bot',
  queryVector,
  'user-123',
  5
);
// Returns top 5 most similar memories
```

---

## ğŸ“Š Stats & Monitoring

### Get Memory Stats

```typescript
const stats = await memory.getStats('my-bot');
console.log(stats);
// {
//   totalVectors: 1240,
//   dimension: 1536
// }
```

### Pinecone Dashboard

https://app.pinecone.io â†’ Your index

**Metrics:**
- Total vectors stored
- Queries per second
- P95 latency
- Index utilization

---

## ğŸ’° Cost Optimization

### 1. Caching (Automatic!)

Embeddings are cached in Redis for 24h:
```
First request:  100ms + $0.00002
Cached request: 5ms + $0 ğŸ’°
```

**Savings:** ~90% on repeated queries

### 2. Batch Processing

```typescript
// Generate embeddings in batch
const results = await embeddings.embedBatch([
  'text 1',
  'text 2',
  'text 3'
]);
// 3x faster + same API cost
```

### 3. Namespace Organization

```typescript
// One namespace per bot
const namespace = `bot-${botId}`;

// Query only relevant namespace
await pinecone.query(
  vector,
  10,
  namespace // Filter to bot's memories only
);
```

---

## ğŸ›¡ï¸ Best Practices

### 1. Store Meaningful Content

**Good:**
```typescript
await memory.store(botId, userId, 'User prefers dark mode');
await memory.storeFact(botId, userId, 'Lives in SÃ£o Paulo', 'location');
```

**Bad:**
```typescript
await memory.store(botId, userId, 'ok'); // Too short, not useful
await memory.store(botId, userId, longText); // Too long, split into chunks
```

### 2. Use Metadata

```typescript
await memory.store(botId, userId, content, {
  type: 'preference',
  category: 'ui',
  confidence: 0.9,
  source: 'explicit'
});

// Later, filter by metadata
const uiPrefs = results.filter(m => 
  m.metadata?.category === 'ui'
);
```

### 3. Regular Cleanup

```typescript
// Delete old/irrelevant memories
await memory.deleteUserMemories(botId, userId);

// Or clear entire bot
await memory.clearBot(botId);
```

---

## ğŸ”§ Advanced Usage

### Custom Similarity Threshold

```typescript
const memories = await memory.search(botId, query, userId, 10);

// Filter by similarity score
const relevant = memories.filter(m => m.score > 0.8); // 80%+
console.log(`Found ${relevant.length} highly relevant memories`);
```

### Combine Recent + Relevant

```typescript
const context = await memory.getContext(
  botId,
  userId,
  query,
  5,  // recent
  10  // relevant
);

// context.recent = Last 5 messages (by time)
// context.relevant = Top 10 similar (by semantics)
// context.summary = Combined formatted string
```

### Batch Storage

```typescript
// Store multiple memories efficiently
await memory.storeBatch(botId, userId, [
  { content: 'Fact 1', metadata: { type: 'fact' } },
  { content: 'Fact 2', metadata: { type: 'fact' } },
  { content: 'Fact 3', metadata: { type: 'fact' } },
]);
// Faster than individual stores
```

---

## ğŸ› Troubleshooting

### "Pinecone not initialized"

**Check 1: Environment variables**
```bash
grep PINECONE .env
# Should show:
# PINECONE_ENABLED=true
# PINECONE_API_KEY=xxx
# PINECONE_INDEX=opencell-memory
```

**Check 2: OpenAI configured**
```bash
grep OPENAI_API_KEY .env
# Required for embeddings
```

**Check 3: Restart**
```bash
npm run build && npm start
```

---

### "Index not found"

**Run setup script:**
```bash
npx tsx scripts/setup-pinecone.ts
```

**Or create manually:**
1. Pinecone dashboard â†’ Create Index
2. Name: `opencell-memory`
3. Dimensions: `1536`
4. Metric: `cosine`
5. Cloud: `aws`
6. Region: `us-east-1`

---

### Slow queries

**Check 1: Index stats**
```typescript
const stats = await pinecone.getStats();
console.log(stats);
// Check: vectorCount, dimension
```

**Check 2: Network latency**
- Use closest region (check Pinecone dashboard)
- Consider dedicated plan for lower latency

**Check 3: Query size**
```typescript
// Reduce topK if not needed
const results = await memory.search(botId, query, userId, 5);
// vs
const results = await memory.search(botId, query, userId, 100); // Slower
```

---

## ğŸ“ˆ ROI Calculation

### Cost Comparison

**Before Pinecone (context limits):**
```
Bot forgets after 200k tokens
User repeats preferences â†’ Wasted tokens
No semantic search â†’ Manual queries
Cost: Lost productivity + user frustration
```

**After Pinecone:**
```
Infinite memory â†’ No context limits
Smart retrieval â†’ Only relevant context
Semantic search â†’ Find anything instantly
Cost: $0-10/month (free tier â†’ starter)

VALUE:
- 50% fewer repeated questions = 100 hours/year saved
- Personalized responses = Better UX
- Context across sessions = Continuity

ROI: ~$2,400/year ğŸ’°
```

### Per-Request Cost

**Embedding generation:**
- $0.00002 per 1k tokens
- Avg message: 50 tokens = $0.000001
- 1M messages = $1

**Pinecone storage:**
- Free tier: 100k vectors (enough for 10k users)
- Starter: $70/month for 5M vectors

**Total monthly cost (1000 users):**
- Embeddings: $5
- Pinecone: $0 (free tier)
- **Total: $5/month = $60/year**

**ROI:** $2,400 value - $60 cost = **$2,340 profit/year** ğŸ‰

---

## ğŸ¯ Next Steps

### Week 1: Setup & Test
1. âœ… Create Pinecone account
2. âœ… Run setup script
3. âœ… Test with sample bot
4. âœ… Check Pinecone dashboard

### Week 2: Production Use
1. Enable in all bots
2. Monitor query performance
3. Optimize metadata filters
4. Measure user satisfaction

### Week 3: Advanced Features
1. Implement custom scoring
2. Add confidence thresholds
3. Create memory cleanup jobs
4. Build memory analytics dashboard

---

## ğŸ“š Resources

- [Pinecone Docs](https://docs.pinecone.io)
- [OpenAI Embeddings](https://platform.openai.com/docs/guides/embeddings)
- [Vector Search Guide](https://www.pinecone.io/learn/vector-search/)
- [RAG Tutorial](https://www.pinecone.io/learn/retrieval-augmented-generation/)

---

**Next:** [Temporal Integration](temporal-guide.md) - Durable Workflows

**Questions?** Check `DOCS_INDEX.md` or open an issue.
