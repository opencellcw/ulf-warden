# Usando Modelos Llama Uncensored

## ðŸ”“ O que sÃ£o modelos Uncensored?

Modelos "uncensored" sÃ£o versÃµes sem filtros de seguranÃ§a, permitindo respostas mais livres sem restriÃ§Ãµes Ã©ticas prÃ©-programadas.

**âš ï¸ AVISO IMPORTANTE:**
- Use com responsabilidade
- Ciente das implicaÃ§Ãµes Ã©ticas
- Respeite leis locais
- NÃ£o recomendado para produÃ§Ã£o pÃºblica

## ðŸŽ¯ RecomendaÃ§Ã£o: Usar Ollama

Para modelos uncensored, **Ollama Ã© a melhor opÃ§Ã£o** porque:
- âœ… Suporta GGUF nativo (formato dos Llamas)
- âœ… Mais rÃ¡pido que transformers.js
- âœ… Maior seleÃ§Ã£o de modelos uncensored
- âœ… FÃ¡cil instalaÃ§Ã£o e gerenciamento
- âœ… Funciona localmente (privacidade total)

## ðŸ“¦ Setup Completo - 5 Passos

### Passo 1: Instalar Ollama

**macOS/Linux:**
```bash
curl -fsSL https://ollama.com/install.sh | sh
```

**Windows:**
Baixe em: https://ollama.com/download

**Verificar:**
```bash
ollama --version
```

### Passo 2: Baixar Modelo Uncensored

**Recomendado para comeÃ§ar:**
```bash
ollama pull wizard-vicuna-uncensored:7b
```

**Outros modelos disponÃ­veis:**
```bash
# Dolphin Mistral (excelente)
ollama pull dolphin-mistral:7b

# Llama 3 Uncensored (mais recente)
ollama pull llama3-uncensored:8b

# Neural Chat (rÃ¡pido)
ollama pull neural-chat-uncensored:7b
```

### Passo 3: Testar

```bash
ollama run wizard-vicuna-uncensored:7b
```

### Passo 4: Configurar Ulf

**Edite `.env`:**
```env
# Habilitar Ollama
OLLAMA_ENABLED=true
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=wizard-vicuna-uncensored:7b

# EstratÃ©gia
LLM_STRATEGY=hybrid
```

### Passo 5: Integrar Provider

O provider Ollama jÃ¡ foi criado em `src/llm/ollama.ts`!

**Agora precisa integrar ao router:**

Edite `src/llm/router.ts` - adicione no inÃ­cio:
```typescript
import { getOllamaProvider } from './ollama';
```

No constructor, adicione:
```typescript
private ollamaProvider: LLMProvider;

constructor() {
  this.claudeProvider = getClaudeProvider();
  this.localProvider = getLocalProvider();
  this.ollamaProvider = getOllamaProvider(); // ADICIONE
  // ...
}
```

Rebuild e restart:
```bash
npm run build
npm start
```

## ðŸŽ® Como Usar

### Modo 1: HÃ­brido (Recomendado)

```env
LLM_STRATEGY=hybrid
OLLAMA_ENABLED=true
```

- Conversas simples â†’ Ollama (uncensored)
- Tarefas complexas â†’ Claude
- Tools â†’ Claude

### Modo 2: Ollama PrioritÃ¡rio

```env
LLM_STRATEGY=local_fallback
OLLAMA_ENABLED=true
```

- Tenta Ollama primeiro
- Fallback para Claude se falhar
- MÃ¡xima liberdade

### Modo 3: Somente Ollama

```env
LLM_STRATEGY=local_only
OLLAMA_ENABLED=true
```

- Todas respostas via Ollama
- Zero custo API
- 100% uncensored

## ðŸ“Š Modelos Recomendados

### â­ Melhor Custo-BenefÃ­cio
**wizard-vicuna-uncensored:7b**
- Tamanho: ~4GB
- RAM: 8GB recomendado
- Velocidade: RÃ¡pido
- Qualidade: Boa

### ðŸ† Melhor Qualidade
**dolphin-mistral:7b**
- Tamanho: ~4.5GB
- RAM: 8GB recomendado
- Velocidade: RÃ¡pido
- Qualidade: Excelente

### ðŸ†• Mais Recente
**llama3-uncensored:8b**
- Tamanho: ~5GB
- RAM: 10GB recomendado
- Velocidade: Moderado
- Qualidade: Excelente

### ðŸ’ª Alta Performance
**wizard-vicuna-uncensored:13b**
- Tamanho: ~8GB
- RAM: 16GB+ recomendado
- Velocidade: Lento
- Qualidade: Melhor possÃ­vel

## ðŸš€ Deploy no Render

**âš ï¸ LimitaÃ§Ã£o:** Ollama requer servidor rodando localmente ou em servidor dedicado.

**OpÃ§Ãµes:**

### OpÃ§Ã£o 1: Desenvolvimento Local
- Ollama roda na sua mÃ¡quina
- Ulf se conecta via localhost
- Melhor para testes

### OpÃ§Ã£o 2: Servidor Dedicado
- Deploy Ollama em VPS separado
- Ulf se conecta via URL
- Melhor para produÃ§Ã£o

```env
OLLAMA_BASE_URL=http://seu-servidor:11434
```

### OpÃ§Ã£o 3: HÃ­brido (Recomendado)
- Desenvolvimento: Ollama local
- ProduÃ§Ã£o: Claude only
- Troque via env var

## ðŸ› Troubleshooting

### "Ollama not available"

```bash
# Iniciar Ollama
ollama serve
```

### "Model not found"

```bash
# Listar modelos
ollama list

# Baixar
ollama pull wizard-vicuna-uncensored:7b
```

### Muito lento

- Use modelo 7B em vez de 13B
- Adicione mais RAM
- Use GPU se disponÃ­vel

## ðŸ“ Resumo RÃ¡pido

```bash
# 1. Instalar
curl -fsSL https://ollama.com/install.sh | sh

# 2. Baixar modelo
ollama pull wizard-vicuna-uncensored:7b

# 3. Testar
ollama run wizard-vicuna-uncensored:7b

# 4. Configurar .env
echo "OLLAMA_ENABLED=true" >> .env
echo "OLLAMA_MODEL=wizard-vicuna-uncensored:7b" >> .env

# 5. Integrar ao router (manual)
# Edite src/llm/router.ts como mostrado acima

# 6. Build e run
npm run build
npm start
```

**Pronto!** Agora Ulf usa Llama uncensored para respostas sem filtros! ðŸŽ‰

## ðŸ”— Links Ãšteis

- [Ollama](https://ollama.com)
- [Modelos](https://ollama.com/library)
- [Wizard Vicuna](https://huggingface.co/TheBloke/Wizard-Vicuna-7B-Uncensored-GGUF)
- [Dolphin Mistral](https://huggingface.co/cognitivecomputations/dolphin-2.2.1-mistral-7b)
