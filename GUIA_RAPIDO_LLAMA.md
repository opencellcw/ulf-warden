# üöÄ Guia R√°pido - Llama Uncensored no Ulf

## ‚úÖ O que foi feito

**Push:** ‚úÖ Feito! C√≥digo est√° no GitHub
**Provider Ollama:** ‚úÖ Criado em `src/llm/ollama.ts`
**Documenta√ß√£o:** ‚úÖ Completa em `docs/UNCENSORED_MODELS.md`

## üéØ Como usar (5 minutos)

### 1Ô∏è‚É£ Instalar Ollama

```bash
# macOS/Linux
curl -fsSL https://ollama.com/install.sh | sh

# Windows: https://ollama.com/download
```

### 2Ô∏è‚É£ Baixar modelo uncensored

```bash
# Recomendado (4GB, r√°pido)
ollama pull wizard-vicuna-uncensored:7b

# OU melhor qualidade (4.5GB)
ollama pull dolphin-mistral:7b
```

### 3Ô∏è‚É£ Testar

```bash
ollama run wizard-vicuna-uncensored:7b
```

### 4Ô∏è‚É£ Configurar Ulf

Adicione no `.env`:

```env
# Habilitar Ollama
OLLAMA_ENABLED=true
OLLAMA_MODEL=wizard-vicuna-uncensored:7b
OLLAMA_BASE_URL=http://localhost:11434

# Estrat√©gia (usar Ollama para respostas simples)
LLM_STRATEGY=hybrid
```

### 5Ô∏è‚É£ Integrar ao Router

**Edite `src/llm/router.ts`:**

```typescript
// No topo do arquivo, adicione:
import { getOllamaProvider } from './ollama';

// No constructor da classe LLMRouter, adicione:
private ollamaProvider: LLMProvider;
private ollamaAvailable: boolean = false;

constructor() {
  this.claudeProvider = getClaudeProvider();
  this.localProvider = getLocalProvider();
  this.ollamaProvider = getOllamaProvider(); // ADICIONE ESTA LINHA

  // ... resto do c√≥digo

  // Adicione tamb√©m:
  this.checkOllamaAvailability();
}

// Adicione este m√©todo:
private async checkOllamaAvailability(): Promise<void> {
  this.ollamaAvailable = await this.ollamaProvider.isAvailable();
  log.info('[Router] Ollama availability checked', {
    available: this.ollamaAvailable
  });
}

// No m√©todo selectProviderHybrid, ANTES de retornar localProvider:
private selectProviderHybrid(taskType: TaskType): LLMProvider {
  const simpleTasksForLocal = [
    TaskType.SIMPLE_CHAT,
    TaskType.TEXT_CLASSIFICATION,
    TaskType.SUMMARIZATION
  ];

  // ADICIONE ESTA VERIFICA√á√ÉO:
  if (simpleTasksForLocal.includes(taskType) && this.ollamaAvailable) {
    log.info('[Router] Using Ollama (hybrid: simple task)', { taskType });
    return this.ollamaProvider;
  }

  // C√≥digo existente...
  if (simpleTasksForLocal.includes(taskType) && this.localAvailable) {
    log.info('[Router] Using local model (hybrid: simple task)', { taskType });
    return this.localProvider;
  }

  log.info('[Router] Using Claude (hybrid: complex task)', { taskType });
  return this.claudeProvider;
}
```

### 6Ô∏è‚É£ Build e Run

```bash
npm run build
npm start
```

## üéÆ Como funciona

**Com hybrid strategy:**

- "Oi!" ‚Üí **Ollama** (uncensored, r√°pido)
- "Como voc√™ est√°?" ‚Üí **Ollama** (sem filtros)
- "Crie uma API FastAPI" ‚Üí **Claude** (complexo)
- "Execute comando" ‚Üí **Claude** (tools)

## üìä Estrat√©gias dispon√≠veis

```env
# Sempre Claude (padr√£o, seguro)
LLM_STRATEGY=claude_only

# H√≠brido - simples=Ollama, complexo=Claude (recomendado)
LLM_STRATEGY=hybrid

# Tenta Ollama primeiro, fallback Claude
LLM_STRATEGY=local_fallback

# Sempre Ollama (sem custos API)
LLM_STRATEGY=local_only
```

## üéØ Modelos Dispon√≠veis

### Come√ßar com (7B):
```bash
ollama pull wizard-vicuna-uncensored:7b
```
- **Tamanho:** ~4GB
- **RAM:** 8GB recomendado
- **Velocidade:** R√°pido
- **Sem filtros:** ‚úÖ

### Melhor qualidade (7B):
```bash
ollama pull dolphin-mistral:7b
```
- **Tamanho:** ~4.5GB
- **RAM:** 8GB recomendado
- **Qualidade:** Excelente
- **Muito criativo:** ‚úÖ

### Llama 3 (8B):
```bash
ollama pull llama3-uncensored:8b
```
- **Tamanho:** ~5GB
- **RAM:** 10GB recomendado
- **Mais recente:** ‚úÖ

## üîß Comandos √öteis

```bash
# Ver modelos instalados
ollama list

# Remover modelo
ollama rm wizard-vicuna-uncensored:7b

# Ver info do Ollama
ollama info

# Iniciar servidor (se n√£o estiver rodando)
ollama serve

# Rodar em background
ollama serve &
```

## ‚ö° Quick Test

Depois de configurar:

```bash
# 1. Certifique-se que Ollama est√° rodando
ollama serve &

# 2. Build
npm run build

# 3. Start Ulf
npm start

# 4. Mande mensagem simples no Slack/Discord
# Deve usar Ollama (sem filtros)

# 5. Cheque os logs
# [Router] Using Ollama (hybrid: simple task)
```

## üêõ Problemas?

### Ollama n√£o conecta
```bash
# Verificar se est√° rodando
ollama list

# Se n√£o, inicie
ollama serve
```

### Modelo n√£o encontrado
```bash
# Baixe novamente
ollama pull wizard-vicuna-uncensored:7b
```

### Muito lento
- Use modelo 7B (n√£o 13B)
- Feche outros programas
- Adicione mais RAM

## üí° Dicas

1. **Privacidade total:** Ollama roda 100% local
2. **Sem custo API:** Gr√°tis ap√≥s download
3. **GPU opcional:** Funciona em CPU, melhor em GPU
4. **M√∫ltiplos modelos:** Pode ter v√°rios instalados
5. **Troca f√°cil:** Mude `OLLAMA_MODEL` no .env

## üìù Resumo Final

**O que voc√™ tem agora:**
- ‚úÖ C√≥digo completo no GitHub (pushed)
- ‚úÖ Provider Ollama pronto (`src/llm/ollama.ts`)
- ‚úÖ Documenta√ß√£o completa
- ‚úÖ Suporte a modelos uncensored
- ‚úÖ Roteamento inteligente
- ‚úÖ 100% local (privacidade)
- ‚úÖ Zero custo API

**Pr√≥ximos passos:**
1. Instale Ollama
2. Baixe modelo uncensored
3. Configure .env
4. Integre ao router (edi√ß√£o manual)
5. Build e test

**Resultado:**
- Respostas sem filtros quando usar Ollama
- Custos de API reduzidos
- Privacidade total (local)
- Flexibilidade (escolhe qual modelo usar)

---

**D√∫vidas?** Veja documenta√ß√£o completa em `docs/UNCENSORED_MODELS.md`

**Pronto para testar!** üöÄüîì
