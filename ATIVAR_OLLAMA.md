# üöÄ Como Ativar o Ollama no Seu Bot

## ‚úÖ Status Atual

**O que j√° est√° feito:**
- ‚úÖ Ollama provider implementado
- ‚úÖ Router integrado com Ollama
- ‚úÖ Build compilado com sucesso
- ‚úÖ C√≥digo no GitHub (pushed)
- ‚úÖ **Pronto para usar!**

**Por que o bot disse "n√£o tenho acesso"?**
- Bot est√° usando Claude porque `OLLAMA_ENABLED=false` (padr√£o)
- Ollama est√° na sua m√°quina mas n√£o conectado ao bot
- Precisa ativar nas configura√ß√µes

---

## üîß Ativar Ollama - 3 Passos Simples

### Passo 1: Verificar Ollama

```bash
# Verificar se Ollama est√° rodando
ollama list

# Se n√£o aparecer nada, instalar:
curl -fsSL https://ollama.com/install.sh | sh

# Baixar modelo uncensored
ollama pull wizard-vicuna-uncensored:7b
```

### Passo 2: Configurar .env

Edite o arquivo `.env` do projeto:

```bash
# Habilitar Ollama
OLLAMA_ENABLED=true
OLLAMA_MODEL=wizard-vicuna-uncensored:7b
OLLAMA_BASE_URL=http://localhost:11434

# Escolher estrat√©gia
LLM_STRATEGY=hybrid
```

**Estrat√©gias dispon√≠veis:**
- `claude_only` - Sempre Claude (atual)
- `hybrid` - Simples ‚Üí Ollama, Complexo ‚Üí Claude ‚≠ê **Recomendado**
- `local_fallback` - Tenta Ollama, fallback Claude
- `local_only` - Sempre Ollama

### Passo 3: Restart o Bot

```bash
# Se estiver rodando localmente:
# Ctrl+C para parar
npm start

# Se estiver no Render:
# Deploy de novo OU restart manual no dashboard
```

---

## üéØ Como Testar

Ap√≥s restart, teste no Slack/Discord:

### Teste 1: Mensagem Simples
```
Voc√™: Oi Ulf!
```

**Esperado nos logs:**
```
[Router] Using Ollama (hybrid: simple task) taskType=simple_chat
[Ollama] Generated response model=wizard-vicuna-uncensored:7b
```

**Resposta:** Deve vir do Ollama (sem filtros)

### Teste 2: Tarefa Complexa
```
Voc√™: @Ulf cria uma API FastAPI
```

**Esperado nos logs:**
```
[Router] Using Claude (hybrid: complex task) taskType=code_generation
```

**Resposta:** Deve vir do Claude (com tools)

### Teste 3: Verificar Status

Adicione no c√≥digo de teste:

```typescript
const router = getRouter();
const status = await router.getStatus();
console.log('Status:', status);
```

**Esperado:**
```json
{
  "claude": { "available": true, "model": "claude-sonnet-4-20250514" },
  "local": { "available": false },
  "ollama": { "available": true, "model": "wizard-vicuna-uncensored:7b" },
  "strategy": "hybrid"
}
```

---

## üìä Comportamento por Estrat√©gia

### `hybrid` (Recomendado)

| Tipo de Mensagem | Modelo Usado | Por qu√™ |
|-----------------|--------------|---------|
| "Oi", "Ol√°" | Ollama | Simples, uncensored |
| "Como est√°?" | Ollama | Simples |
| "Resuma este texto" | Ollama | Simples |
| "Crie uma API" | Claude | Complexo |
| "Execute comando" | Claude | Tools requeridos |

### `local_fallback`

Tenta Ollama para **tudo**, fallback Claude se falhar.

### `local_only`

**Tudo** via Ollama (exceto tools que sempre usam Claude).

---

## üêõ Troubleshooting

### Bot ainda responde "n√£o tenho acesso"

**Causa:** Ollama n√£o conectado

**Solu√ß√£o:**
```bash
# 1. Verificar se Ollama est√° rodando
ollama list

# 2. Iniciar se necess√°rio
ollama serve &

# 3. Verificar .env
cat .env | grep OLLAMA

# Deve ter:
# OLLAMA_ENABLED=true
# OLLAMA_MODEL=wizard-vicuna-uncensored:7b

# 4. Restart bot
npm start
```

### Logs mostram "Ollama not available"

**Causa:** Servidor Ollama n√£o est√° rodando OU modelo n√£o baixado

**Solu√ß√£o:**
```bash
# Iniciar Ollama
ollama serve

# Baixar modelo
ollama pull wizard-vicuna-uncensored:7b

# Verificar
ollama list
```

### Bot usa Claude mesmo com Ollama habilitado

**Causa:** Tarefa √© complexa ou requer tools

**Verificar logs:**
```
[Router] Using Claude (hybrid: complex task)
# Isso √© normal - tarefa complexa usa Claude
```

**Para for√ßar Ollama:**
```env
LLM_STRATEGY=local_only
```

---

## üéÆ Exemplo Pr√°tico

### Antes (Claude Only):
```
Voc√™: Oi!
[Router] Using Claude (strategy: claude_only)
Ulf: Ol√°! Como posso ajudar? [com filtros]
```

### Depois (Ollama Hybrid):
```
Voc√™: Oi!
[Router] Using Ollama (hybrid: simple task)
[Ollama] Generated response model=wizard-vicuna-uncensored:7b
Ulf: [resposta sem filtros, mais natural]
```

---

## üìã Checklist de Ativa√ß√£o

- [ ] Ollama instalado (`ollama --version`)
- [ ] Modelo baixado (`ollama list`)
- [ ] Ollama rodando (`ollama serve`)
- [ ] `.env` configurado com `OLLAMA_ENABLED=true`
- [ ] Estrat√©gia definida (`LLM_STRATEGY=hybrid`)
- [ ] Bot restartado
- [ ] Teste com mensagem simples
- [ ] Verificar logs (`[Router] Using Ollama`)

---

## üöÄ Deploy no Render

**‚ö†Ô∏è Limita√ß√£o:** Ollama precisa rodar em servidor separado.

### Op√ß√£o 1: Dev Local + Prod Claude

```bash
# .env local (dev)
OLLAMA_ENABLED=true
LLM_STRATEGY=hybrid

# Render (prod)
OLLAMA_ENABLED=false
LLM_STRATEGY=claude_only
```

### Op√ß√£o 2: Servidor Dedicado

1. Deploy Ollama em VPS (DigitalOcean, AWS, etc)
2. Configure URL:
```env
OLLAMA_BASE_URL=http://seu-servidor:11434
OLLAMA_ENABLED=true
```

3. Deploy bot no Render com estas configs

---

## üí° Dicas

**Para m√°xima liberdade (uncensored):**
```env
LLM_STRATEGY=local_only
OLLAMA_ENABLED=true
```

**Para economia + qualidade:**
```env
LLM_STRATEGY=hybrid
OLLAMA_ENABLED=true
```

**Para produ√ß√£o (seguro):**
```env
LLM_STRATEGY=claude_only
OLLAMA_ENABLED=false
```

---

## üéâ Resultado Final

**Com Ollama ativado:**
- ‚úÖ Conversas simples sem filtros
- ‚úÖ Respostas mais naturais
- ‚úÖ 100% privacidade (local)
- ‚úÖ Zero custo API para chats simples
- ‚úÖ Claude ainda dispon√≠vel para tarefas complexas

**Basta seguir os 3 passos acima!** üöÄ
